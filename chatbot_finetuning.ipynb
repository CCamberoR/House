{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fvCzDQnpb-68",
   "metadata": {
    "id": "fvCzDQnpb-68"
   },
   "source": [
    "# Ajuste fino del modelo `tiny-llama`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9_iwJ5k5bvFp",
   "metadata": {
    "id": "9_iwJ5k5bvFp"
   },
   "source": [
    "**ESTE NOTEBOOK NO FUNCIONARÁ EN UN MAC OS POR LOS REQUISITOS TÉCNICOS DEL TRAINER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8b16578cd7dc34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T16:59:57.362396Z",
     "start_time": "2025-01-18T16:59:56.872785Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e8b16578cd7dc34",
    "outputId": "3efac86d-0a2b-4a44-9ebd-78d3d8ca81a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face logging\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "token = \"huggingface_token\"\n",
    "print(\"Hugging Face logging\")\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7dcc2801542a58a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T16:59:57.369883Z",
     "start_time": "2025-01-18T16:59:57.366935Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7dcc2801542a58a",
    "outputId": "87cd5b5b-bc1b-4f6f-87c3-aea7690ed34c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "iYpZQNCpcJ_0",
   "metadata": {
    "id": "iYpZQNCpcJ_0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
      "Collecting trl\n",
      "  Downloading trl-0.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.9)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.1.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.7.1)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.47.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (6.1.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.2.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (0.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.34.0->trl) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate>=0.34.0->trl) (12.8.61)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate>=0.34.0->trl) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->accelerate>=0.34.0->trl) (1.3.0)\n",
      "Downloading trl-0.14.0-py3-none-any.whl (313 kB)\n",
      "Installing collected packages: trl\n",
      "Successfully installed trl-0.14.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef73d09e98622764",
   "metadata": {
    "id": "ef73d09e98622764"
   },
   "source": [
    "## Cargar el modelo + tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0e071d98173f19c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T17:06:48.270390Z",
     "start_time": "2025-01-18T17:06:41.785935Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0e071d98173f19c",
    "outputId": "e04672da-6819-488e-ec9e-f9dae0b118e1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d34dd29bdc14490b5db9d78988752b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462647595787424bad5841be291ae2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d627965c04544787bfe2a8a04cf76210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17df73ce1fc4f8fb37f6f95e6dc9087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cc88140b2b4bb5b4b4d25be76a0039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649288512d9841bfbe0664c91d68e711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a35b7468cf1450c86df202d0715a7b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36db9ddde834c45a5c2074ae9f640eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be9b5d731a64a43867bc104133f5af6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2f6ebe770f46528ccef5b49f9f2dff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1a7a0c5be84e9eae6d4d626a1817b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6288eb08cf4e8d8ed5968e00a07209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a52141ea9d40489d469d9a15b68497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "cache_dir = \"./models/microsoft/phi-2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float32 if device == \"mps\" else (torch.float16 if torch.cuda.is_available() else torch.float32),\n",
    "            cache_dir=cache_dir,\n",
    "            local_files_only=False,\n",
    "        ).to(device)\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16522e60c88175c",
   "metadata": {
    "id": "a16522e60c88175c"
   },
   "source": [
    "## Cargar el dataset y preprocesar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T17:00:05.728395Z",
     "start_time": "2025-01-18T17:00:03.538664Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120,
     "referenced_widgets": [
      "82f323e0aeca46f3955a76515354de8f",
      "6f2fe803b68d4cc293df292f3ed07cde",
      "182b0ee0432446208c2e66838777f023",
      "661b295b83a54452a07ebe324942f1ad",
      "c66be6cb91ff41089f9f57f2b4bd334e",
      "fdee23af289247779d0041258fb54100",
      "d8e434e7562a4550aaed00ee5bf970e2",
      "e28a6850e09049f5979e63a83e5c752a",
      "af307f04a81a415a806c71181030bab9",
      "091b7ff1c0c0409f9793cc175d041f7b",
      "43a5cb03d28c425cbf282226b492a525"
     ]
    },
    "collapsed": true,
    "id": "initial_id",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d5f350ec-b468-4118-e8f1-39e3271bf7fa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9aff7568a24c94af2acef567b41296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/106556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['Conversation', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 106556\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Asignar el token de padding\n",
    "\n",
    "# Load local dataset\n",
    "# Change the path and format according to your data file\n",
    "# Supported formats: csv, json, txt, parquet\n",
    "dataset = load_dataset('json', data_files={\n",
    "    'train': 'medicare/medicare_110k_train.json',\n",
    "    'test': 'medicare/medicare_110k_test.json'\n",
    "})['train']  # We only use train for fine-tuning\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    max_length = 2048  # Máxima longitud para Phi-2\n",
    "\n",
    "    # Convertir conversaciones al formato esperado\n",
    "    conversations = []\n",
    "    for text in examples['Conversation']:\n",
    "        dialogue = []\n",
    "        parts = text.split('[|')\n",
    "        for part in parts[1:]:  # Saltar el primer elemento vacío\n",
    "            role, content = part.split('|]')\n",
    "            role = role.strip().lower()\n",
    "            content = content.strip()\n",
    "            if role == 'human':\n",
    "                dialogue.append({\"role\": \"user\", \"content\": content})\n",
    "            elif role == 'ai':\n",
    "                dialogue.append({\"role\": \"assistant\", \"content\": content})\n",
    "    \n",
    "        # Aplicar el chat template para formatear el diálogo\n",
    "        formatted_text = tokenizer.apply_chat_template(\n",
    "            dialogue,\n",
    "            tokenize=False,\n",
    "            return_tensors=None\n",
    "        )\n",
    "        conversations.append(formatted_text)\n",
    "    \n",
    "    # Tokenizar las conversaciones formateadas\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"Conversation\"],\n",
    "        padding=\"max_length\",  # O usa \"longest\" si tienes más memoria\n",
    "        truncation=True,\n",
    "        max_length=512,  # Ajusta según el modelo\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return tokenized\n",
    "\n",
    "# Aplicar preprocesamiento al dataset\n",
    "dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22f2fed92127161",
   "metadata": {
    "id": "d22f2fed92127161"
   },
   "source": [
    "## Configurar y ejecutar el entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d947bc2bbd1d2b",
   "metadata": {
    "id": "24d947bc2bbd1d2b"
   },
   "source": [
    "Configurar PEFT (ajuste fino eficiente de parámetros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "717125f5bdb1264",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T17:02:27.443803Z",
     "start_time": "2025-01-18T17:02:27.438646Z"
    },
    "id": "717125f5bdb1264"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e2613f659da4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-18T17:08:47.964312Z",
     "start_time": "2025-01-18T17:08:47.465980Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1b1e2613f659da4b",
    "outputId": "adf47c3c-7fa4-443d-fc94-58d44eef8219"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 23\u001b[0m\n\u001b[1;32m      7\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      8\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      9\u001b[0m     gradient_accumulation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Usa FP16 si tienes compatibilidad con CUDA\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(\n\u001b[1;32m     19\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     20\u001b[0m     mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m )\n\u001b[0;32m---> 23\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# peft_config=lora_config,\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Liberar memoria antes de entrenar\u001b[39;00m\n\u001b[1;32m     32\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/trl/trainer/sft_trainer.py:236\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[0m\n\u001b[1;32m    233\u001b[0m             peft_module_casting_to_bf16(model)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m processing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     processing_class \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39m_name_or_path)\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(processing_class, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpad_token\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m         processing_class\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m processing_class\u001b[38;5;241m.\u001b[39meos_token\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "# Solución para mejorar tokenización y manejo de memoria\n",
    "from transformers import TrainingArguments, DataCollatorForLanguageModeling\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "\n",
    "# Configuración de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    remove_unused_columns=False,  # Asegura que se pasen las columnas correctas\n",
    "    output_dir=\"./output\",\n",
    "    logging_steps=10,\n",
    "    max_steps=1000,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,  # Usa FP16 si tienes compatibilidad con CUDA\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=lora_config,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Entrenamiento\n",
    "trainer.train()\n",
    "trainer.save_model(\"./models/phi-2-FT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tcYXCDGzcXS3",
   "metadata": {
    "id": "tcYXCDGzcXS3"
   },
   "source": [
    "# Uso de los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "djNrlIyccizf",
   "metadata": {
    "id": "djNrlIyccizf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class DialogueController:\n",
    "\n",
    "    def __init__(self, system_prompt, assistant_token=\"<|assistant|>\", keep_cronology=True):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.prompt = [ { \"role\": \"system\", \"content\": system_prompt} ]\n",
    "        self.assistant_token = assistant_token\n",
    "        self.keep_cronology = keep_cronology\n",
    "\n",
    "    def get_prompt(self):\n",
    "        return self.prompt\n",
    "\n",
    "    def add_user_prompt(self, user_prompt, role=\"user\"):\n",
    "        user_prompt_json = { \"role\": role, \"content\":  user_prompt}\n",
    "        if self.keep_cronology:\n",
    "          self.prompt.append(user_prompt_json)\n",
    "        else:\n",
    "          self.prompt = [self.prompt[0], user_prompt_json]\n",
    "        return self.prompt\n",
    "\n",
    "    def add_assistant_prompt(self, assistant_prompt):\n",
    "        if self.keep_cronology:\n",
    "          assistant_prompt_exclusive = assistant_prompt.split(self.assistant_token)\n",
    "          final_prompt = \"\"\n",
    "          if len(assistant_prompt_exclusive) == 2:\n",
    "              final_prompt = assistant_prompt_exclusive[1]\n",
    "          self.add_user_prompt(final_prompt, role=\"assistant\")\n",
    "\n",
    "\n",
    "class Chatbot2:\n",
    "\n",
    "    def __init__(self, controller, chatbot_model=\"microsoft/phi-2\", cache_dir=\"./microsoft/phi-2\"):\n",
    "        global device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(chatbot_model)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            chatbot_model,\n",
    "            torch_dtype=torch.float32 if device == \"mps\" else (torch.float16 if torch.cuda.is_available() else torch.float32),\n",
    "            cache_dir=cache_dir,\n",
    "            local_files_only=False,\n",
    "        ).to(device)\n",
    "        self.model_device = device\n",
    "        self.dialogue_controller = controller\n",
    "\n",
    "    def __run_prompt(self, prompt, do_sample, temperature, top_p, max_length, show_prompt):\n",
    "        formatted_prompt = self.tokenizer.apply_chat_template(conversation=prompt, tokenize=False, return_dict=False, add_generation_prompt=True)\n",
    "\n",
    "        # Tokenizar\n",
    "        inputs = self.tokenizer(formatted_prompt, max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(self.model_device) for k, v in inputs.items()}\n",
    "\n",
    "        # Muestra infomacion de log\n",
    "        if show_prompt:\n",
    "          print(formatted_prompt)\n",
    "          print(\"--- Token size: ---\")\n",
    "          [print(\"\\t\", k, \": \", len(v[0])) for k, v in inputs.items()]\n",
    "          print(\"-------------------\")\n",
    "\n",
    "        # Generar respuesta\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Decodificar y limpiar respuesta\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    def answer(self, user_prompt, do_sample=True, temperature=0.1, top_p=0.9, max_length=2047, show_prompt=False):\n",
    "        # Actualiza el prompt\n",
    "        prompt = self.dialogue_controller.add_user_prompt(user_prompt)\n",
    "        # Resolver prompt\n",
    "        answer = self.__run_prompt(prompt, do_sample, temperature, top_p, max_length, show_prompt)\n",
    "        # Actualiza el prompt con la respuesta del asistente\n",
    "        self.dialogue_controller.add_assistant_prompt(answer)\n",
    "        return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "477bd2d05715ce2b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "477bd2d05715ce2b",
    "outputId": "36dc3240-1e49-4ba4-fff7-56e6a98e9e01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular:  <|system|>\n",
      "Responde mis dudas de manera coincisa, breve, y siempre en español. En caso de no conocer la respuesta dime 'no lo se'. \n",
      "<|user|>\n",
      "Cuál es la capital de francia? \n",
      "<|assistant|>\n",
      "La capital de Francia es París.\n",
      "fine tuned:  <|system|>\n",
      "Responde mis dudas de manera coincisa, breve, y siempre en español. En caso de no conocer la respuesta dime 'no lo se'. \n",
      "<|user|>\n",
      "Cuál es la capital de francia? \n",
      "<|assistant|>\n",
      "La capital de Francia es París.\n",
      "****expected answer:  Paris\n",
      "----------------------\n",
      "regular:  <|system|>\n",
      "Responde mis dudas de manera coincisa, breve, y siempre en español. En caso de no conocer la respuesta dime 'no lo se'. \n",
      "<|user|>\n",
      "¿Qué juegos me recomendarías si me ha gustado Undertale? \n",
      "<|assistant|>\n",
      "Sure, Undertale is a great game to play if you enjoyed the aesthetic and gameplay of Toby Fox's Undertale. Here are some games that you might enjoy:\n",
      "\n",
      "1. Celeste: This game is a platformer with a unique twist - the gameplay is based on the player's breathing. It's a challenging and rewarding game that's perfect for fans of Undertale.\n",
      "\n",
      "2. Celeste: Dark Moon: This is a sequel to Celeste that adds new levels and challenges to the game. It's a great way to continue the story and explore the game's world.\n",
      "\n",
      "3. Super Mario Odyssey: This game is a 3D platformer that features a unique and colorful world. It's a great game for fans of Undertale's art style and gameplay.\n",
      "\n",
      "4. Super Mario Odyssey: The Legend of Zelda: Breath of the Wild: This game is a spiritual successor to The Legend of Zelda: A Link to the Past and Breath of the Wild. It's a great game for fans of Undertale's art style and gameplay.\n",
      "\n",
      "5. Super Mario Odyssey: The Legend of Zelda: Breath of the Wild: This game is a spiritual successor to The Legend of Zelda: A Link to the Past and Breath of the Wild. It's a great game for fans of Undertale's art style and gameplay.\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n",
      "fine tuned:  <|system|>\n",
      "Responde mis dudas de manera coincisa, breve, y siempre en español. En caso de no conocer la respuesta dime 'no lo se'. \n",
      "<|user|>\n",
      "¿Qué juegos me recomendarías si me ha gustado Undertale? \n",
      "<|assistant|>\n",
      "Sure, Undertale is a great game to play if you enjoyed the gameplay mechanics and storyline of Toby Fox's Undertale. Here are some games that you might enjoy:\n",
      "\n",
      "1. Celeste: This game is a challenging platformer that features a unique gameplay mechanic where the player controls a character as they climb a mountain.\n",
      "\n",
      "2. Super Mario Odyssey: This game is a 3D platformer where the player controls Mario and can explore different worlds and collect items.\n",
      "\n",
      "3. Super Mario Bros.: This classic game is a 2D platformer where the player controls Mario and can explore different levels and collect coins.\n",
      "\n",
      "4. Super Mario Bros. 3: This game is a 2D platformer where the player controls Mario and can explore different levels and collect coins.\n",
      "\n",
      "5. Super Mario Bros. 2: This game is a 2D platformer where the player controls Mario and can explore different levels and collect coins.\n",
      "\n",
      "6. Super Mario Bros. 35: This game is a 2D platformer where the player controls Mario and can explore different levels and collect coins.\n",
      "\n",
      "7. Super Mario Bros. 3: This game is a 2D platformer where the player controls Mario and can explore different levels and collect coins.\n",
      "\n",
      "8. Super Mario Bros.: This classic game is a 2D platformer where the player controls Mario and can explore different levels and collect coins.\n",
      "\n",
      "9. Super Mario Bros. 35: This game is a 2D platformer where the player controls Mario and can explore different levels and collect coins.\n",
      "\n",
      "10. Super Mario Bros. 3: This game is a 2D platformer where the player controls Mario and can explore different levels and collect coins.\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n",
      "****expected answer:  Deltarune, EarthBound, Hollow Knight, Hyper Light Drifter, Ori and the Blind Forest\n",
      "----------------------\n",
      "regular:  <|system|>\n",
      "Responde mis dudas de manera coincisa, breve, y siempre en español. En caso de no conocer la respuesta dime 'no lo se'. \n",
      "<|user|>\n",
      "¡Hola! ¿Me podrías ayudar con un problema? No me encuentro del todo bien y me duele bastante la cabeza, ¿Sabes que podría ser? \n",
      "<|assistant|>\n",
      "Sure, I can help you with that.\n",
      "\n",
      "I understand that you may feel a bit overwhelmed and stressed out. It's normal to have doubts and questions at times.\n",
      "\n",
      "Here are some tips to help you cope with your headaches:\n",
      "\n",
      "1. Take a break: Sometimes, taking a break from studying or work can help you clear your mind and reduce stress.\n",
      "\n",
      "2. Practice relaxation techniques: There are many relaxation techniques you can try, such as deep breathing, meditation, or yoga. These techniques can help you calm your mind and reduce stress.\n",
      "\n",
      "3. Seek support: If you're feeling overwhelmed, it's okay to seek support from friends, family, or a mental health professional. They can help you identify your concerns and provide you with the support you need.\n",
      "\n",
      "4. Practice self-care: Make sure to take care of yourself physically and emotionally. This could mean getting enough sleep, eating a healthy diet, and exercising regularly.\n",
      "\n",
      "5. Stay positive: Try to stay positive and focus on the positive aspects of your life. This can help you feel more confident and motivated.\n",
      "\n",
      "Remember, it's okay to have doubts and questions. Don't be afraid to ask for help or seek support. You're not alone in this.\n",
      "fine tuned:  <|system|>\n",
      "Responde mis dudas de manera coincisa, breve, y siempre en español. En caso de no conocer la respuesta dime 'no lo se'. \n",
      "<|user|>\n",
      "¡Hola! ¿Me podrías ayudar con un problema? No me encuentro del todo bien y me duele bastante la cabeza, ¿Sabes que podría ser? \n",
      "<|assistant|>\n",
      "Sure, I can help you with that. Here's a possible solution:\n",
      "\n",
      "1. \"¿No te duerme?\" (Do you sleep well?)\n",
      "2. \"¿No te duerme?\" (Do you sleep well?)\n",
      "3. \"¿No te duerme?\" (Do you sleep well?)\n",
      "\n",
      "In this example, the question is asked in Spanish, and the response is \"No\" in the same way that it would be in English.\n",
      "\n",
      "Remember that the tone and style of the question and response should be the same in both languages. If you're unsure about the tone or style, it's always best to ask a native speaker for guidance.\n",
      "****expected answer:  Puede que estés sufriendo de un dolor de cabeza. El dolor de cabeza puede ser causado por muchas cosas, desde estrés hasta una lesión. Si el dolor de cabeza persiste o empeora, es importante consultar a un médico para descartar cualquier afección grave. Algunas cosas que puedes hacer para aliviar el dolor de cabeza incluyen tomar analgésicos, evitar el estrés, descansar, practicar técnicas de relajación y evitar los desencadenantes como la cafeína, el alcohol y los cambios repentinos de presión.\n",
      "----------------------\n",
      "regular:  <|system|>\n",
      "Responde mis dudas de manera coincisa, breve, y siempre en español. En caso de no conocer la respuesta dime 'no lo se'. \n",
      "<|user|>\n",
      "quien era Francisco Morazán? \n",
      "<|assistant|>\n",
      "Francisco Morazán (1802-1853) was a Mexican statesman, diplomat, and revolutionary leader who played a significant role in the Mexican War of Independence (1810-1821) and the Mexican Revolution (1910-1920). He was the first president of the Republic of Mexico and served as the first president of the Mexican Empire (1821-1823). Morazán was a member of the Liberal Party and was known for his advocacy for democracy, human rights, and national unity. He was also a proponent of the idea of a federal system for Mexico, which was eventually implemented in the country's constitution of 1917.\n",
      "fine tuned:  <|system|>\n",
      "Responde mis dudas de manera coincisa, breve, y siempre en español. En caso de no conocer la respuesta dime 'no lo se'. \n",
      "<|user|>\n",
      "quien era Francisco Morazán? \n",
      "<|assistant|>\n",
      "Francisco Morazán (1802-1850) was a Mexican general, politician, and revolutionary leader who played a significant role in the Mexican War of Independence and the Mexican Revolution. He was born in the town of Morazán, in the state of Chiapas, and was the son of a wealthy landowner. Morazán joined the army at a young age and rose through the ranks to become a general in the Mexican army. He played a key role in the Mexican War of Independence, where he led the army in several battles against the Spanish Empire. After the war, Morazán became a prominent figure in Mexican politics, serving as the president of the Mexican Republic from 1857 to 1861. He was also a prominent figure in the Mexican Revolution, where he played a key role in the Battle of Puebla and the subsequent fall of the Spanish Empire.\n",
      "****expected answer:  Francisco Morazán fue un líder político y militar hondureño que trabajó para unir a los países de Centroamérica en una sola nación durante la primera mitad del siglo XIX. Fue elegido presidente de Honduras y de la Confederación Centroamericana y promovió la educación, la justicia y la igualdad. A pesar de ser derrocado y exiliado varias veces, Morazán es recordado como un héroe nacional en Honduras y en otros países de Centroamérica.\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# Modelos disponibles\n",
    "fine_tuned_model = \"./models/phi-2-FT\"\n",
    "model_name = \"microsoft/phi-2\"\n",
    "cache_dir = \"./models/microsoft/phi-2\"\n",
    "\n",
    "# Cargar los modelos a través del chatbot\n",
    "dialog_controller1 = DialogueController(system_prompt=\"Responde mis dudas de manera coincisa, breve, y siempre en español. En caso de no conocer la respuesta dime 'no lo se'.\",keep_cronology=False)\n",
    "dialog_controller2 = DialogueController(system_prompt=\"Responde mis dudas de manera coincisa, breve, y siempre en español. En caso de no conocer la respuesta dime 'no lo se'.\",keep_cronology=False)\n",
    "\n",
    "chatbot_finetuned = Chatbot2(controller=dialog_controller1, chatbot_model=model_name, cache_dir=fine_tuned_model)\n",
    "chatbot_standard = Chatbot2(controller=dialog_controller2)\n",
    "\n",
    "questions = [\n",
    "    (\"Me duele la cabeza, ¿qué puedo hacer?\", \"Debes tomar un analgésico y descansar.\"),\n",
    "    (\"He comido algo en mal estado, ¿qué debo hacer?\", \"Debes acudir a un médico.\"),\n",
    "    (\"¿Tengo un esguince en la rodilla, qué debo hacer?\", \"Debes aplicar hielo y reposar.\"),\n",
    "    (\"¿Qué debo hacer si tengo fiebre?\", \"Debes tomar un antipirético.\"),\n",
    "    (\"¿Qué es la diabetes?\", \"La diabetes es una enfermedad crónica que afecta a la forma en que el cuerpo convierte los alimentos en energía.\"),\n",
    "    (\"¿Qué es la hipertensión?\", \"La hipertensión es una enfermedad crónica que se caracteriza por una presión arterial elevada.\"),\n",
    "    (\"¿Qué es el colesterol?\", \"El colesterol es una sustancia cerosa que se encuentra en la sangre.\"),\n",
    "    (\"¿Qué es el cáncer?\", \"El cáncer es una enfermedad en la que las células anormales se dividen sin control y pueden invadir otros tejidos.\"),\n",
    "    (\"¿Qué es la artritis?\", \"La artritis es una enfermedad que causa dolor e inflamación en las articulaciones.\"),\n",
    "    (\"¿Qué es la osteoporosis?\", \"La osteoporosis es una enfermedad en la que los huesos se vuelven frágiles y propensos a fracturas.\"),\n",
    "    (\"¿Qué es el asma?\", \"El asma es una enfermedad crónica que afecta las vías respiratorias.\"),\n",
    "    (\"¿Qué es la migraña?\", \"La migraña es un tipo de dolor de cabeza que puede ser intenso y debilitante.\"),\n",
    "    (\"¿Qué es la depresión?\", \"La depresión es un trastorno del estado de ánimo que causa una sensación persistente de tristeza y pérdida de interés.\"),\n",
    "    (\"¿Qué es la ansiedad?\", \"La ansiedad es una emoción de miedo o preocupación excesiva.\"),\n",
    "    (\"¿Qué es el estrés?\", \"El estrés es una respuesta física y emocional a un estímulo que causa tensión.\"),\n",
    "    (\"¿Qué es el Alzheimer?\", \"El Alzheimer es una enfermedad cerebral progresiva que afecta la memoria y la capacidad de pensar.\"),\n",
    "    (\"¿Qué es el Parkinson?\", \"El Parkinson es una enfermedad del sistema nervioso que afecta el movimiento.\"),\n",
    "    (\"¿Qué es la esclerosis múltiple?\", \"La esclerosis múltiple es una enfermedad autoinmune que afecta el sistema nervioso.\"),\n",
    "]\n",
    "for question, answer in questions:\n",
    "    print(\"regular: \",chatbot_standard.answer(user_prompt=question))\n",
    "    print(\"fine tuned: \",chatbot_finetuned.answer(user_prompt=question))\n",
    "    print(\"****expected answer: \", answer)\n",
    "    print(\"----------------------\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "091b7ff1c0c0409f9793cc175d041f7b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "182b0ee0432446208c2e66838777f023": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e28a6850e09049f5979e63a83e5c752a",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_af307f04a81a415a806c71181030bab9",
      "value": 1000
     }
    },
    "43a5cb03d28c425cbf282226b492a525": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "661b295b83a54452a07ebe324942f1ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_091b7ff1c0c0409f9793cc175d041f7b",
      "placeholder": "​",
      "style": "IPY_MODEL_43a5cb03d28c425cbf282226b492a525",
      "value": " 1000/1000 [00:01&lt;00:00, 657.37 examples/s]"
     }
    },
    "6f2fe803b68d4cc293df292f3ed07cde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fdee23af289247779d0041258fb54100",
      "placeholder": "​",
      "style": "IPY_MODEL_d8e434e7562a4550aaed00ee5bf970e2",
      "value": "Map: 100%"
     }
    },
    "82f323e0aeca46f3955a76515354de8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6f2fe803b68d4cc293df292f3ed07cde",
       "IPY_MODEL_182b0ee0432446208c2e66838777f023",
       "IPY_MODEL_661b295b83a54452a07ebe324942f1ad"
      ],
      "layout": "IPY_MODEL_c66be6cb91ff41089f9f57f2b4bd334e"
     }
    },
    "af307f04a81a415a806c71181030bab9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c66be6cb91ff41089f9f57f2b4bd334e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8e434e7562a4550aaed00ee5bf970e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e28a6850e09049f5979e63a83e5c752a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdee23af289247779d0041258fb54100": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
