{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "75fffec23aa77053",
      "metadata": {
        "id": "75fffec23aa77053"
      },
      "source": [
        "Los sistemas RAG permiten mejorar la eficacia de los sistemas de preguntas respuestas (query answering). Por un lado reducen las alucinaciones que los modelos puedan tener y, por otro lado, permiten dar una evidencia empírica de la proveniencia de la información sobre la que se basa la respuesta. Para ello, los sistemas RAG necesitan una base documental sobre la que articular la respuesta. Cuando hablamos de documentos, no nos referimos a ficheros en sí, sino a conjuntos de información como pueden ser párrafos, frases, páginas de texto, etc. dependiendo de la granularidad que se le quiera dar a dicha unidad. El flujo del modelo RAG, dada una consulta y una base documental, sería:\n",
        "\n",
        "* Retrieval: buscar fragmentos relevantes en la base documental que tengan relación con la pregunta.\n",
        "* Augmentation: pasar los fragmentos recuperados como contexto adicional al modelo generador.\n",
        "* Generation: el modelo genera la respuesta teniendo en cuenta el contexto documental\n",
        "\n",
        "A continuación vamos a ver varias opciones para la parte de Retrieval (Recuperación), como comparar dichos sistemas, y la mejora que incorporan al módulo de generación.\n",
        "\n",
        "Para ello, vamos a reutilizar parte del código visto en el notebook de [Búsqueda Dispersa y Densa](https://github.com/cbadenes/curso-pln/blob/main/notebooks/08_Busqueda_Dispersa_y_Densa.ipynb) y en el de [RAG Avanzado](https://github.com/cbadenes/curso-pln/blob/main/notebooks/08_RAG_Avanzado.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xmFyCbztWY_X",
      "metadata": {
        "id": "xmFyCbztWY_X"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bfe622de738220c",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-18T19:55:05.890876Z",
          "start_time": "2025-01-18T19:55:05.454062Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bfe622de738220c",
        "outputId": "e16e9d8b-9b9f-4c52-cd73-f822a625d2cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hugging Face logging\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "token = \"huggingface_token\"\n",
        "print(\"Hugging Face logging\")\n",
        "\n",
        "login(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2701253e0f710414",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-18T19:55:17.820368Z",
          "start_time": "2025-01-18T19:55:05.897065Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2701253e0f710414",
        "outputId": "d1c92644-66d5-4fb2-f161-b1aa683aef94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "device_setup= \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using: \",device_setup)\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc93e1a0641bb997",
      "metadata": {
        "id": "dc93e1a0641bb997"
      },
      "source": [
        "# RAG: Retrieval-Augmented Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-18T19:55:30.023854Z",
          "start_time": "2025-01-18T19:55:17.956833Z"
        },
        "collapsed": true,
        "id": "initial_id",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from typing import List, Tuple\n",
        "import nltk\n",
        "\n",
        "# Descarga de recursos necesarios\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "class TextPreprocessor:\n",
        "\n",
        "    @classmethod\n",
        "    def preprocess(cls, text: str, lang='english') -> str:\n",
        "        if isinstance(text, tuple): text = ' '.join(text)\n",
        "        text = text.lower()\n",
        "        tokens = word_tokenize(text)\n",
        "        stop_words = set(stopwords.words(lang))\n",
        "        tokens = [t for t in tokens if t not in stop_words]\n",
        "\n",
        "        return ' '.join(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "afb884c7c7a806c2",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-18T19:55:30.034962Z",
          "start_time": "2025-01-18T19:55:30.031017Z"
        },
        "id": "afb884c7c7a806c2"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class Retriever(ABC):\n",
        "\n",
        "    def __init__(self, name='abstract_retriever'):\n",
        "        self.name = name\n",
        "\n",
        "    def get_name(self):\n",
        "        return self.name\n",
        "\n",
        "    \"\"\"\n",
        "    Este método recibe un conjunto de documentos y los indexa para poder realizar búsquedas posteriores\n",
        "    \"\"\"\n",
        "    @abstractmethod\n",
        "    def build_index(self, documents: List[str], lang: str = 'english'):\n",
        "        pass\n",
        "\n",
        "    \"\"\"\n",
        "        Este método búsca los documentos relevantes para una query.\n",
        "        Devuelve una lista con el la posición (index) del documento encontrado y su score de relevancia.\n",
        "    \"\"\"\n",
        "    @abstractmethod\n",
        "    def search(self, query: str, top_k: int = 3, lang:str = 'english') -> List[Tuple[int, float]]:\n",
        "        pass\n",
        "\n",
        "    \"\"\"\n",
        "        Este método búsca los documentos relevantes para una query.\n",
        "        Devuelve los documentos que considera relevantes.\n",
        "    \"\"\"\n",
        "    @abstractmethod\n",
        "    def search_documents(self, query: str, top_k: int = 3, lang:str = 'english') -> List[str]:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76b9178e3605c6c3",
      "metadata": {
        "id": "76b9178e3605c6c3"
      },
      "source": [
        "## SparseRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "87d4b9b3a1e73a3a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-18T19:55:30.284437Z",
          "start_time": "2025-01-18T19:55:30.041747Z"
        },
        "id": "87d4b9b3a1e73a3a"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "\n",
        "'''\n",
        "    * Búsqueda eficiente: El uso de NearestNeighbors con una métrica de similitud como el coseno permite realizar búsquedas rápidas.\n",
        "    * TF-IDF como base: Las palabras más relevantes en cada documento obtienen un peso mayor, mejorando la precisión de la búsqueda.\n",
        "'''\n",
        "class SparseRetrieverNM(Retriever):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__(\"sparse_retriever_nm\")\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.nn_model = NearestNeighbors(n_neighbors=5, metric=\"cosine\", algorithm=\"auto\")\n",
        "\n",
        "    \"\"\"\n",
        "    Construye el índice usando TF-IDF\n",
        "    \"\"\"\n",
        "    def build_index(self, documents: List[str], lang: str = 'english'):\n",
        "        self.documents = documents\n",
        "        # Limpiar tokens innecesarios\n",
        "        processed_docs = [TextPreprocessor.preprocess(doc, lang) for doc in self.documents]\n",
        "        # Generar embeddings dispersos TF-IDF\n",
        "        self.tfidf_matrix = self.vectorizer.fit_transform(processed_docs)\n",
        "        # Construir un modelo de búsqueda eficiente\n",
        "        self.nn_model.fit(self.tfidf_matrix)\n",
        "\n",
        "    def search(self, query: str, top_k: int = 5, lang: str = 'english') -> List[Tuple[int, float]]:\n",
        "        # Vectorizar la consulta\n",
        "        processed_query = TextPreprocessor.preprocess(query, lang)\n",
        "        query_vector = self.vectorizer.transform([processed_query])\n",
        "\n",
        "        # Encontrar los vecinos más cercanos\n",
        "        distances, indices = self.nn_model.kneighbors(query_vector, n_neighbors=top_k)\n",
        "        # Retornar resultados como documentos y distancias inversas (para similitud)\n",
        "        return [(idx, score) for idx, score in zip(indices[0], distances[0])][::-1]\n",
        "\n",
        "    def search_documents(self, query: str, top_k: int = 3, lang: str = 'english') -> List[str]:\n",
        "        relevant_documents = self.search(query, top_k, lang)\n",
        "        return [self.documents[idx] for idx, score in relevant_documents]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9bbde3b8b4669a8e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-18T19:55:30.306583Z",
          "start_time": "2025-01-18T19:55:30.300351Z"
        },
        "id": "9bbde3b8b4669a8e"
      },
      "outputs": [],
      "source": [
        "class SparseRetriever(Retriever):\n",
        "    def __init__(self):\n",
        "        super().__init__('sparse_retriever')\n",
        "\n",
        "    def build_index(self, documents: List[str], lang:str = 'english'):\n",
        "         self.documents = documents\n",
        "         processed_docs = [TextPreprocessor.preprocess(doc, lang) for doc in self.documents]\n",
        "         tokenized_docs = [doc.split() for doc in processed_docs]\n",
        "         self.bm25 = BM25Okapi(tokenized_docs)\n",
        "\n",
        "\n",
        "    def search(self, query: str, top_k: int = 3, lang:str = 'english') -> List[Tuple[int, float]]:\n",
        "        processed_query = TextPreprocessor.preprocess(query, lang)\n",
        "        query_tokens = processed_query.split()\n",
        "        scores = self.bm25.get_scores(query_tokens)\n",
        "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
        "        return [(idx, scores[idx]) for idx in top_indices]\n",
        "\n",
        "    def search_documents(self, query: str, top_k: int = 3, lang:str = 'english') -> List[str]:\n",
        "        relevant_documents = self.search(query, top_k, lang)\n",
        "        return [self.documents[idx] for idx, score in relevant_documents]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d58ff96fdcf7bd6",
      "metadata": {
        "id": "7d58ff96fdcf7bd6"
      },
      "source": [
        "## Dense Retriever:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e33596aa3ad668cd",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-18T19:55:30.321989Z",
          "start_time": "2025-01-18T19:55:30.316078Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e33596aa3ad668cd",
        "outputId": "2e8ba3ed-4ca9-4c2e-cbc5-46cc9ac4ac3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-06 13:01:43.708685: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-02-06 13:01:43.721027: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1738846903.732163       9 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1738846903.735904       9 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-02-06 13:01:43.749815: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# TODO: implementar el DenseRetriever\n",
        "class DenseRetriever(Retriever):\n",
        "\n",
        "    def __init__(self, model='sentence-transformers/all-MiniLM-L6-v2'):\n",
        "        super().__init__('dense_retriever' + model)\n",
        "        # Cargar modelo de embeddings multilingüe\n",
        "        self.model = SentenceTransformer(model)\n",
        "\n",
        "\n",
        "    def build_index(self, documents: List[str], lang: str = 'english'):\n",
        "        # Guardar documentos originales\n",
        "        self.documents = documents\n",
        "        #  Procesar texto eliminando tokens no relevantes\n",
        "        processed_docs = [TextPreprocessor.preprocess(doc, lang) for doc in self.documents]\n",
        "        # Generar y almacenar embeddings\n",
        "        self.embeddings = self.model.encode(processed_docs, show_progress_bar=True)\n",
        "\n",
        "\n",
        "    def search(self, query: str, top_k: int = 3, lang: str = 'english') -> List[Tuple[int, float]]:\n",
        "        # Realiza búsqueda por similitud de embeddings.\n",
        "        processed_query = TextPreprocessor.preprocess(query, lang)\n",
        "        # Generar embedding de la query\n",
        "        query_embedding = self.model.encode([processed_query])\n",
        "        # Calcular similitud\n",
        "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
        "        # Obtener top_k resultados\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "        return [(idx, similarities[idx]) for idx in top_indices]\n",
        "\n",
        "\n",
        "    def search_documents(self, query: str, top_k: int = 3, lang: str = 'english') -> List[str]:\n",
        "        relevant_documents = self.search(query, top_k, lang)\n",
        "        return [self.documents[idx] for idx, score in relevant_documents]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f0ff00b7f6653fcc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-18T19:55:30.332014Z",
          "start_time": "2025-01-18T19:55:30.326122Z"
        },
        "id": "f0ff00b7f6653fcc"
      },
      "outputs": [],
      "source": [
        "class HybridRetriever(Retriever):\n",
        "\n",
        "    def __init__(self, weight_sparse: float = 0.3,\n",
        "                 weight_dense: float = 0.7, model='sentence-transformers/all-MiniLM-L6-v2'):\n",
        "        super().__init__('hybrid_retriever' + model)\n",
        "        self.model = model\n",
        "        self.weight_sparse = weight_sparse\n",
        "        self.weight_dense = weight_dense\n",
        "\n",
        "    def build_index(self, documents: List[str], lang: str = 'english'):\n",
        "        self.sparse_retriever = SparseRetriever()\n",
        "        self.dense_retriever = DenseRetriever(self.model)\n",
        "        self.sparse_retriever.build_index(documents)\n",
        "        self.dense_retriever.build_index(documents)\n",
        "        self.documents = documents\n",
        "\n",
        "    def search(self, query: str, top_k: int = 3, lang: str = 'english') -> List[Tuple[int, float]]:\n",
        "        # Obtener resultados de ambos retrievers\n",
        "        sparse_results = self.sparse_retriever.search(query, top_k=top_k, lang=lang)\n",
        "        dense_results = self.dense_retriever.search(query, top_k=top_k, lang=lang)\n",
        "\n",
        "        # Combinar scores\n",
        "        combined_scores = {}\n",
        "        for idx, score in sparse_results:\n",
        "            combined_scores[idx] = score * self.weight_sparse\n",
        "\n",
        "        for idx, score in dense_results:\n",
        "            if idx in combined_scores:\n",
        "                combined_scores[idx] += score * self.weight_dense\n",
        "            else:\n",
        "                combined_scores[idx] = score * self.weight_dense\n",
        "\n",
        "        # Ordenar resultados finales\n",
        "        sorted_results = sorted(combined_scores.items(),\n",
        "                              key=lambda x: x[1],\n",
        "                              reverse=True)[:top_k]\n",
        "        # Preparar resultados\n",
        "        return [(idx, score) for idx, score in sorted_results]\n",
        "\n",
        "    def search_documents(self, query: str, top_k: int = 3, lang: str = 'english') -> List[str]:\n",
        "        relevant_documents = self.search(query, top_k, lang)\n",
        "        return [self.documents[idx] for idx, score in relevant_documents]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4df3a091500e61d0",
      "metadata": {
        "id": "4df3a091500e61d0"
      },
      "source": [
        "## Evaluación de sistemas RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e973b0927a251956",
      "metadata": {
        "id": "e973b0927a251956"
      },
      "source": [
        "Para evaluar estos sistemas, necesitamos un dataset que posea una pregunta, un conjunto de textos, y una relación entre la pregunta y cuáles de dichos textos son 'relevantes' para responder la pregunta. Para ello, podemos usar los datasets de `rungalileo/ragbench`, por ejemplo el `techqa`. Vamos a escribir el código para que por un lado se extraigan todos los posibles documentos del dataset y, por otro lado, se emparejen las consultas con los fragmentos relevantes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e447cf58714cad0e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-18T19:55:35.013094Z",
          "start_time": "2025-01-18T19:55:30.339811Z"
        },
        "id": "e447cf58714cad0e"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "import re\n",
        "\n",
        "def preprocess_conversation(text):\n",
        "    text = text.strip()\n",
        "    # Separar las intervenciones del humano y la IA\n",
        "    parts = re.split(r\"\\[\\|Human\\|\\]|\\[\\|AI\\|\\]\", text)\n",
        "    parts = [p.strip() for p in parts]\n",
        "\n",
        "    if len(parts) != 3:\n",
        "        return None, None  # Manejar conversaciones mal formateadas\n",
        "\n",
        "    human_text = parts[1]\n",
        "    ai_text = parts[2]\n",
        "\n",
        "    return human_text, ai_text\n",
        "\n",
        "def load_and_preprocess_data(dataset): # Ya no recibe un filepath, sino el dataset cargado\n",
        "    \"\"\"Carga el dataset (ya cargado) y preprocesa las conversaciones.\"\"\"\n",
        "    conversations = []\n",
        "    print(len(dataset))\n",
        "    for i, conv in enumerate(dataset):  # Iterar directamente sobre el dataset cargado\n",
        "        human_text, ai_text = preprocess_conversation(conv['Conversation'])\n",
        "        conversations.append((human_text, ai_text))\n",
        "    return conversations\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "58d1f99c",
      "metadata": {
        "id": "58d1f99c"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "def save_retriever(retriever, save_dir='saved_retriever'):\n",
        "    # Create directory if it doesn't exist\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    # Save the documents\n",
        "    with open(os.path.join(save_dir, 'documents.pkl'), 'wb') as f:\n",
        "        pickle.dump(retriever.documents, f)\n",
        "\n",
        "    # Save sparse retriever data\n",
        "    with open(os.path.join(save_dir, 'sparse_bm25.pkl'), 'wb') as f:\n",
        "        pickle.dump(retriever.sparse_retriever.bm25, f)\n",
        "\n",
        "    # Save dense retriever embeddings\n",
        "    with open(os.path.join(save_dir, 'dense_embeddings.pkl'), 'wb') as f:\n",
        "        pickle.dump(retriever.dense_retriever.embeddings, f)\n",
        "\n",
        "    # Save model name and weights\n",
        "    config = {\n",
        "        'model_name': retriever.model,\n",
        "        'weight_sparse': retriever.weight_sparse,\n",
        "        'weight_dense': retriever.weight_dense\n",
        "    }\n",
        "    with open(os.path.join(save_dir, 'config.pkl'), 'wb') as f:\n",
        "        pickle.dump(config, f)\n",
        "\n",
        "    print(f\"Retriever saved in {save_dir}\")\n",
        "\n",
        "def load_retriever(save_dir='saved_retriever'):\n",
        "    # Load config\n",
        "    with open(os.path.join(save_dir, 'config.pkl'), 'rb') as f:\n",
        "        config = pickle.load(f)\n",
        "\n",
        "    # Initialize retriever with saved config\n",
        "    retriever = HybridRetriever(\n",
        "        weight_sparse=config['weight_sparse'],\n",
        "        weight_dense=config['weight_dense'],\n",
        "        model=config['model_name']\n",
        "    )\n",
        "\n",
        "    # Load documents\n",
        "    with open(os.path.join(save_dir, 'documents.pkl'), 'rb') as f:\n",
        "        documents = pickle.load(f)\n",
        "\n",
        "    # Initialize index\n",
        "    retriever.build_index(documents)\n",
        "\n",
        "    # Load sparse retriever data\n",
        "    with open(os.path.join(save_dir, 'sparse_bm25.pkl'), 'rb') as f:\n",
        "        retriever.sparse_retriever.bm25 = pickle.load(f)\n",
        "\n",
        "    # Load dense embeddings\n",
        "    with open(os.path.join(save_dir, 'dense_embeddings.pkl'), 'rb') as f:\n",
        "        retriever.dense_retriever.embeddings = pickle.load(f)\n",
        "\n",
        "    print(f\"Retriever loaded from {save_dir}\")\n",
        "    return retriever\n",
        "\n",
        "# Example usage:\n",
        "# Save the current retriever\n",
        "if not os.path.exists('saved_retriever'):\n",
        "    retriever = HybridRetriever(weight_sparse=0.3, weight_dense=0.7, model='sentence-transformers/all-MiniLM-L6-v2')\n",
        "    retriever.build_index(dataset)\n",
        "    save_retriever(retriever)\n",
        "\n",
        "# Load the saved retriever\n",
        "# loaded_retriever = load_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a-OM5feTA4y-",
      "metadata": {
        "id": "a-OM5feTA4y-"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TextStreamer\n",
        "\n",
        "class Chatbot:\n",
        "    def __init__(self, retriever, model_name=\"microsoft/Phi-3-mini-4k-instruct\", load_in_8bit=False, load_in_4bit=True):\n",
        "        self.retriever = retriever\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            trust_remote_code=True,\n",
        "            device_map=device_setup,  #  Importante:  usar device_map\n",
        "\n",
        "        )\n",
        "        self.pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_new_tokens=100,  # Ajusta según necesidad\n",
        "            do_sample=True,\n",
        "            temperature=0.5,\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            repetition_penalty=1.2,\n",
        "            streamer=TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True) # Para ver la respuesta en tiempo real\n",
        "        )\n",
        "\n",
        "    def generate_prompt(self, query: str, context_documents: List[str]) -> str:\n",
        "        \"\"\"Genera el prompt para Phi-3.\"\"\"\n",
        "        context_documents = [doc if isinstance(doc, str) else ' '.join(doc) for doc in context_documents]\n",
        "\n",
        "        context = \"\\n\".join(context_documents)\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "            <|system|>\n",
        "                Eres un asistente de análisis de información médica. Tu función es analizar la consulta del usuario y, *basándote exclusivamente en la información proporcionada en el contexto de consultas médicas anteriores relativas a otros pacientes que te sirven de experiencia previa*, identificar posibles condiciones médicas que *podrían* estar relacionadas con los síntomas descritos.  **Importante:**\n",
        "                *   **No eres un médico y no estás proporcionando un diagnóstico médico.**\n",
        "                *   **La información que proporcionas es solo para fines informativos y de orientación, y NO debe interpretarse como un diagnóstico definitivo.**\n",
        "                *   **Siempre se debe buscar el consejo de un profesional médico cualificado para obtener un diagnóstico y tratamiento adecuados.**\n",
        "                *   **Si la información en el contexto no es suficiente para identificar posibles condiciones, indica claramente: \"No tengo suficiente información para ofrecer posibles condiciones relacionadas.  Es fundamental que consultes a un profesional médico para una evaluación completa.\"**\n",
        "                * Debes responder de forma estructurada, siguiendo este formato (y solo este formato):\n",
        "                1. **Análisis de la consulta**: Breve resumen de los síntomas y la pregunta principal del usuario.\n",
        "                2. **Posibles condiciones (basadas en el contexto):** Lista de *posibles* condiciones médicas que, *según el contexto proporcionado*, podrían estar relacionadas.  Usa un lenguaje condicional (\"podría ser\", \"es posible que\", \"se asemeja a\"). *No* afirmes categóricamente.  Si hay varias posibilidades, ordénalas de más a menos probable *según la frecuencia con la que aparecen en el contexto*.\n",
        "                3. **Advertencia:** Reiteración de que esto NO es un diagnóstico y que se debe consultar a un médico.\n",
        "\n",
        "                Responde en español.\n",
        "                </s>\n",
        "                <|user|>\n",
        "                Contexto:\n",
        "                {context}\n",
        "\n",
        "                Consulta del paciente: {query}\n",
        "                </s>\n",
        "                <|assistant|>\n",
        "                **Respuesta de un doctor en menos de 100 palabras**:\n",
        "        \"\"\"\n",
        "        return prompt\n",
        "\n",
        "\n",
        "    def chat(self, query: str, lang: str = 'english', top_k: int = 3):\n",
        "        \"\"\"Función principal de chat (adaptada para streaming).\"\"\"\n",
        "        relevant_docs = self.retriever.search_documents(query, top_k=top_k, lang=lang)\n",
        "        prompt = self.generate_prompt(query, relevant_docs)\n",
        "\n",
        "        # Usamos .generate() directamente para tener más control (streaming y length_penalty)\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(device_setup)\n",
        "        streamer = TextStreamer(self.tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
        "\n",
        "        #  Generamos en un hilo separado para no bloquear la interfaz\n",
        "        with st.spinner('Generando respuesta...'):  # Muestra un spinner mientras genera\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=128,  #  ¡Ajusta! Valor inicial bajo\n",
        "                repetition_penalty=1.2,\n",
        "                #length_penalty=1.5,   #  Opcional: descomenta para usar length_penalty\n",
        "                do_sample=True,\n",
        "                top_k=50,\n",
        "                top_p=0.95,\n",
        "                streamer=streamer,  #  Usa el streamer\n",
        "            )\n",
        "\n",
        "        #  Extraemos la respuesta completa (ya que el streamer la imprimió)\n",
        "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        #  Aislar la respuesta del chatbot (quitando el prompt)\n",
        "        response = full_response.replace(prompt, \"\").strip()\n",
        "\n",
        "\n",
        "        # ---  ACTUALIZA EL HISTORIAL  ---\n",
        "        self.conversation_history += f\"<|user|>\\nConsulta del paciente: {query}\\n</s>\\n<|assistant|>\\n{response}\\n</s>\\n\"\n",
        "\n",
        "        return response\n",
        "    def clear_history(self):\n",
        "        \"\"\"Limpia el historial de la conversación.\"\"\"\n",
        "        self.conversation_history = \"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "FZelRoo_BURG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FZelRoo_BURG",
        "outputId": "63c63db3-1ba1-4a9d-b5be-140eb78bd532"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-02-06 13:12:03.799 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-06 13:12:03.800 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-06 13:12:03.800 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-06 13:12:03.801 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-06 13:12:03.801 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-06 13:12:03.801 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-06 13:12:03.802 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-06 13:12:03.802 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-06 13:12:03.803 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-06 13:12:03.804 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-06 13:12:03.804 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-06 13:12:03.804 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-06 13:12:04.304 Thread 'Thread-13': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-06 13:12:04.305 Thread 'Thread-13': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-06 13:14:01.637 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-02-06 13:14:01.638 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m st\u001b[38;5;241m.\u001b[39mchat_message(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     71\u001b[0m             st\u001b[38;5;241m.\u001b[39mwrite(response)  \u001b[38;5;66;03m# Mostramos la respuesta\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[26], line 27\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretriever\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m st\u001b[38;5;241m.\u001b[39msession_state:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_retriever\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m         st\u001b[38;5;241m.\u001b[39msession_state\u001b[38;5;241m.\u001b[39mretriever \u001b[38;5;241m=\u001b[39m \u001b[43mload_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m         retriever \u001b[38;5;241m=\u001b[39m HybridRetriever()\n",
            "Cell \u001b[0;32mIn[12], line 49\u001b[0m, in \u001b[0;36mload_retriever\u001b[0;34m(save_dir)\u001b[0m\n\u001b[1;32m     46\u001b[0m     documents \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Initialize index\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Load sparse retriever data\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_bm25.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
            "Cell \u001b[0;32mIn[8], line 14\u001b[0m, in \u001b[0;36mHybridRetriever.build_index\u001b[0;34m(self, documents, lang)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_retriever \u001b[38;5;241m=\u001b[39m DenseRetriever(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_retriever\u001b[38;5;241m.\u001b[39mbuild_index(documents)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense_retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocuments \u001b[38;5;241m=\u001b[39m documents\n",
            "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mDenseRetriever.build_index\u001b[0;34m(self, documents, lang)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocuments \u001b[38;5;241m=\u001b[39m documents\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#  Procesar texto eliminando tokens no relevantes\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m processed_docs \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mTextPreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Generar y almacenar embeddings\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencode(processed_docs, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocuments \u001b[38;5;241m=\u001b[39m documents\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#  Procesar texto eliminando tokens no relevantes\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m processed_docs \u001b[38;5;241m=\u001b[39m [\u001b[43mTextPreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocuments]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Generar y almacenar embeddings\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencode(processed_docs, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[3], line 20\u001b[0m, in \u001b[0;36mTextPreprocessor.preprocess\u001b[0;34m(cls, text, lang)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mtuple\u001b[39m): text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(text)\n\u001b[1;32m     19\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[0;32m---> 20\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(lang))\n\u001b[1;32m     22\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n",
            "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py:143\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 144\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m ]\n",
            "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/destructive.py:161\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    158\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPUNCTUATION:\n\u001b[0;32m--> 161\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mregexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubstitution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Handles parentheses.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m regexp, substitution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPARENS_BRACKETS\n",
            "File \u001b[0;32m/usr/lib/python3.11/re/__init__.py:315\u001b[0m, in \u001b[0;36m_subx\u001b[0;34m(pattern, template)\u001b[0m\n\u001b[1;32m    312\u001b[0m     template \u001b[38;5;241m=\u001b[39m _parser\u001b[38;5;241m.\u001b[39mparse_template(template, pattern)\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parser\u001b[38;5;241m.\u001b[39mexpand_template(template, match)\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_subx\u001b[39m(pattern, template):\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;66;03m# internal: Pattern.sub/subn implementation helper\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     template \u001b[38;5;241m=\u001b[39m _compile_repl(template, pattern)\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m template[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(template[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;66;03m# literal replacement\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(page_title=\"Asistente Médico Virtual\", page_icon=\"⚕️\")\n",
        "    st.title(\"Asistente Médico Virtual ⚕️\")\n",
        "    st.write(\"Bienvenido.  Este asistente te proporciona información basada en consultas médicas anteriores.  **No sustituye a un profesional médico.**\")\n",
        "\n",
        "    # --- Carga de datos y modelo (una sola vez) ---\n",
        "    with st.spinner('Cargando datos y modelo...'):\n",
        "        # Carga el dataset (solo si no se ha cargado ya)\n",
        "        if 'dataset' not in st.session_state:\n",
        "            path = \"data/\"\n",
        "            ragbench = load_dataset(\n",
        "                \"json\",\n",
        "                data_files=[\n",
        "                    path + \"medicare_110k_train.json\",\n",
        "                    path + \"medicare_110k_test.json\",\n",
        "                ],\n",
        "                split=\"train\"\n",
        "            )\n",
        "            st.session_state.dataset = load_and_preprocess_data(ragbench)\n",
        "\n",
        "\n",
        "        # Carga el retriever (o lo crea y guarda si no existe)\n",
        "        if 'retriever' not in st.session_state:\n",
        "            if os.path.exists('saved_retriever'):\n",
        "                st.session_state.retriever = load_retriever()\n",
        "            else:\n",
        "                retriever = HybridRetriever()\n",
        "                retriever.build_index(st.session_state.dataset, lang='english')\n",
        "                save_retriever(retriever) #Lo guardamos para la próxima\n",
        "                st.session_state.retriever = retriever\n",
        "\n",
        "        # Inicializa el chatbot\n",
        "        if 'chatbot' not in st.session_state:\n",
        "            st.session_state.chatbot = Chatbot(st.session_state.retriever)\n",
        "\n",
        "    chatbot = st.session_state.chatbot  # Accede al chatbot desde el estado de la sesión\n",
        "\n",
        "    # --- Barra lateral (opciones) ---\n",
        "    with st.sidebar:\n",
        "        st.header(\"Opciones\")\n",
        "        top_k = st.slider(\"Número de documentos a recuperar (top_k)\", min_value=1, max_value=10, value=3)\n",
        "        if st.button(\"Limpiar historial\"):\n",
        "            chatbot.clear_history()\n",
        "            st.success(\"Historial limpiado.\")\n",
        "        st.write(\"---\")\n",
        "        st.write(\"**Advertencia:** Este es un prototipo. La información proporcionada no debe considerarse un diagnóstico médico. Consulta siempre a un profesional de la salud.\")\n",
        "\n",
        "\n",
        "    # --- Interfaz principal (chat) ---\n",
        "\n",
        "    # Muestra el historial de la conversación\n",
        "    for message in chatbot.conversation_history.split(\"</s>\\n\"): # Separamos por los tokens de fin\n",
        "        if message.strip():  # Evita mensajes vacíos\n",
        "            if \"<|user|>\" in message:\n",
        "                with st.chat_message('user'):\n",
        "                    st.write(message.replace(\"<|user|>\", \"\").replace(\"Consulta del paciente:\",\"\").strip())\n",
        "            elif \"<|assistant|>\" in message:\n",
        "                with st.chat_message('assistant'):\n",
        "                    st.write(message.replace(\"<|assistant|>\", \"\").strip())\n",
        "\n",
        "    # Input del usuario\n",
        "    user_query = st.chat_input(\"Escribe tu consulta aquí...\")\n",
        "\n",
        "    if user_query:  #  Si el usuario ha escrito algo\n",
        "        with st.chat_message(\"user\"):\n",
        "            st.write(user_query) # Mostramos su pregunta\n",
        "        response = chatbot.chat(user_query, top_k=top_k) # Obtenemos la respuesta\n",
        "        with st.chat_message(\"assistant\"):\n",
        "            st.write(response)  # Mostramos la respuesta\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2f8f3c26fe562f6",
      "metadata": {
        "id": "c2f8f3c26fe562f6"
      },
      "source": [
        "Por último vamos a escribir un método que compruebe la eficacia de un conjunto de retrievers. Nótese que, al contrario que pasaba con los query answering, para los Retrievers es posible calcular métricas no difusas (fuzzy)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
